{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\nengo_dl\\version.py:58: UserWarning: This version of NengoDL has not been tested with your Nengo version (4.0.0). The latest fully supported version is 3.2.0.\n",
      "  warnings.warn(warnstr)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nengo\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tkinter as tk\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nengo_extras.data import one_hot_from_labels\n",
    "from nengo_extras.matplotlib import tile\n",
    "from nengo_extras.vision import Gabor, Mask\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "import pickle\n",
    "import nengo_dl\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import os\n",
    "from random import shuffle\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = 160\n",
    "input_height = 60\n",
    "n_hid = 1000\n",
    "\n",
    "lookup = dict()\n",
    "reverselookup = dict()\n",
    "count = 0\n",
    "for j in os.listdir('./input/leapgestrecog/leapGestRecog/00/'):\n",
    "    if not j.startswith('.'): # If running this code locally, this is to \n",
    "                              # ensure you aren't reading in hidden folders\n",
    "        lookup[j] = count\n",
    "        reverselookup[count] = j\n",
    "        count = count + 1\n",
    "lookup\n",
    "\n",
    "\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "datacount = 0 # We'll use this to tally how many images are in our dataset\n",
    "for i in range(0, 10): # Loop over the ten top-level folders\n",
    "    for j in os.listdir('./input/leapgestrecog/leapGestRecog/0' + str(i) + '/'):\n",
    "        if not j.startswith('.'): # Again avoid hidden folders\n",
    "            count = 0 # To tally images of a given gesture\n",
    "            for k in os.listdir('./input/leapgestrecog/leapGestRecog/0' + \n",
    "                                str(i) + '/' + j + '/'):\n",
    "                                # Loop over the images\n",
    "                img = Image.open('./input/leapgestrecog/leapGestRecog/0' + \n",
    "                                 str(i) + '/' + j + '/' + k).convert('L')\n",
    "                                # Read in and convert to greyscale\n",
    "              \n",
    "                x_data.append(img) \n",
    "                count = count + 1\n",
    "            y_values = np.full((count, 1), lookup[j]) \n",
    "            y_data.append(y_values)\n",
    "            datacount = datacount + count\n",
    "\n",
    "y_data = np.array(y_data)\n",
    "y_data = y_data.reshape(datacount)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.reshape((x_data.shape[0], 1, -1))\n",
    "y_data = y_data.reshape((y_data.shape[0], 1, -1))\n",
    "x_train,x_further,y_train,y_further = train_test_split(x_data,y_data,test_size = 0.2, random_state=42)\n",
    "x_validate,x_test,y_validate,y_test = train_test_split(x_further,y_further,test_size = 0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img_cnn_nengo(img, width=160, height =60) :\n",
    "\n",
    "    img = img.resize((input_width, input_height))\n",
    "    img = np.array(img, dtype = 'float32')\n",
    "    img = img/ 255 * 2 - 1\n",
    "\n",
    "    img = img.reshape((1, 1, -1))\n",
    "\n",
    "\n",
    "    tiled_test_images = np.tile(img, (1, 30, 1))\n",
    "    return tiled_test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img_cnn_nengo(nengo_converter, nengo_input, nengo_output, tiled_test_images, params_file = \"keras_to_snn_big_params\") :\n",
    " \n",
    "        with nengo_dl.Simulator(\n",
    "                nengo_converter.net, minibatch_size=1, progress_bar=False\n",
    "        ) as nengo_sim:\n",
    "                nengo_sim.load_params(params_file)\n",
    "                data = nengo_sim.predict({nengo_input: tiled_test_images})\n",
    "                \n",
    "        predictions = np.argmax(data[nengo_output][:, -1], axis=-1)\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nengo_cnn(tiled_test_images, params_file = \"keras_to_snn_big_params\") :\n",
    "    # input\n",
    "    inp = tf.keras.Input(shape=(input_height, input_width, 1))\n",
    "\n",
    "    # convolutional layers\n",
    "    conv0 = tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation=tf.nn.relu,\n",
    "    )(inp)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        strides=2,\n",
    "        activation=tf.nn.relu,\n",
    "    )(conv0)\n",
    "\n",
    "    # fully connected layer\n",
    "    flatten = tf.keras.layers.Flatten()(conv1)\n",
    "    dense = tf.keras.layers.Dense(units=10)(flatten)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=dense)\n",
    "\n",
    "    nengo_converter = nengo_dl.Converter(\n",
    "        model,\n",
    "        swap_activations={tf.nn.relu: nengo.SpikingRectifiedLinear()},\n",
    "        scale_firing_rates=100,\n",
    "        synapse=0.01,\n",
    "    )\n",
    "\n",
    "    nengo_input = nengo_converter.inputs[inp]\n",
    "    nengo_output = nengo_converter.outputs[dense]\n",
    "\n",
    "    with nengo_converter.net:\n",
    "        nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    return nengo_converter, nengo_input, nengo_output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\nengo_dl\\simulator.py:456: UserWarning: No GPU support detected. See https://www.nengo.ai/nengo-dl/installation.html#installing-tensorflow for instructions on setting up TensorFlow with GPU support.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "tiled_test_images = preprocess_img_cnn_nengo(x_data[3])\n",
    "nengo_converter, nengo_input, nengo_output = load_nengo_cnn(tiled_test_images)\n",
    "prediction = predict_img_cnn_nengo(nengo_converter, nengo_input, nengo_output, tiled_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 9600)\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "tiled_test_images = preprocess_img_cnn_nengo(x_data[12544])\n",
    "\n",
    "prediction = predict_img_cnn_nengo(nengo_converter, nengo_input, nengo_output, tiled_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NengoCNNPredictor:\n",
    "    def __init__(self, params_file=\"keras_to_snn_big_params\"):\n",
    "        self.params_file = params_file\n",
    "        self.loaded = False\n",
    "        self.nengo_converter = None\n",
    "        self.nengo_input = None\n",
    "        self.nengo_output = None\n",
    "        self.nengo_sim = None\n",
    "\n",
    "    def load_model(self):\n",
    "        if not self.loaded:\n",
    "            # Define the model architecture\n",
    "            # input\n",
    "            inp = tf.keras.Input(shape=(input_height, input_width, 1))\n",
    "\n",
    "            # convolutional layers\n",
    "            conv0 = tf.keras.layers.Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=3,\n",
    "                activation=tf.nn.relu,\n",
    "            )(inp)\n",
    "\n",
    "            conv1 = tf.keras.layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                strides=2,\n",
    "                activation=tf.nn.relu,\n",
    "            )(conv0)\n",
    "\n",
    "            # fully connected layer\n",
    "            flatten = tf.keras.layers.Flatten()(conv1)\n",
    "            dense = tf.keras.layers.Dense(units=10)(flatten)\n",
    "\n",
    "            model = tf.keras.Model(inputs=inp, outputs=dense)\n",
    "\n",
    "            nengo_converter = nengo_dl.Converter(\n",
    "                model,\n",
    "                swap_activations={tf.nn.relu: nengo.SpikingRectifiedLinear()},\n",
    "                scale_firing_rates=100,\n",
    "                synapse=0.01,\n",
    "            )\n",
    "\n",
    "            nengo_input = nengo_converter.inputs[inp]\n",
    "            nengo_output = nengo_converter.outputs[dense]\n",
    "\n",
    "            # Convert the model to Nengo\n",
    "            self.nengo_converter = nengo_dl.Converter(\n",
    "                model,\n",
    "                swap_activations={tf.nn.relu: nengo.SpikingRectifiedLinear()},\n",
    "                scale_firing_rates=100,\n",
    "                synapse=0.01,\n",
    "            )\n",
    "\n",
    "            self.nengo_input = self.nengo_converter.inputs[inp]\n",
    "            self.nengo_output = self.nengo_converter.outputs[dense]\n",
    "\n",
    "            with self.nengo_converter.net:\n",
    "                nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "            self.loaded = True\n",
    "\n",
    "    def load_simulator(self):\n",
    "        if not self.loaded:\n",
    "            self.load_model()\n",
    "\n",
    "        self.nengo_sim = nengo_dl.Simulator(self.nengo_converter.net, minibatch_size=1, progress_bar=False)\n",
    "        self.nengo_sim.load_params(self.params_file)\n",
    "\n",
    "    def predict(self, tiled_test_images):\n",
    "        if self.nengo_sim is None:\n",
    "            self.load_simulator()\n",
    "\n",
    "        data = self.nengo_sim.predict({self.nengo_input: tiled_test_images})\n",
    "\n",
    "        predictions = np.argmax(data[self.nengo_output][:, -1], axis=-1)\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 9600)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "predictor = NengoCNNPredictor()\n",
    "tiled_test_images = preprocess_img_cnn_nengo(x_data[12544])\n",
    "predictions = predictor.predict(tiled_test_images)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 9600)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "tiled_test_images = preprocess_img_cnn_nengo(x_data[12])\n",
    "predictions = predictor.predict(tiled_test_images)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 9600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\nengo_dl\\simulator.py:456: UserWarning: No GPU support detected. See https://www.nengo.ai/nengo-dl/installation.html#installing-tensorflow for instructions on setting up TensorFlow with GPU support.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "SimulatorClosed",
     "evalue": "Cannot call predict after simulator is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSimulatorClosed\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tiled_test_images \u001b[38;5;241m=\u001b[39m preprocess_img_cnn_nengo(x_data[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      2\u001b[0m nengo_sim, nengo_input, nengo_output \u001b[38;5;241m=\u001b[39m load_nengo_cnn()\n\u001b[1;32m----> 3\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_img_cnn_nengo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnengo_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnengo_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnengo_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiled_test_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mpredict_img_cnn_nengo\u001b[1;34m(nengo_sim, nengo_input, nengo_output, tiled_test_images)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_img_cnn_nengo\u001b[39m(nengo_sim, nengo_input, nengo_output, tiled_test_images) :\n\u001b[1;32m----> 3\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mnengo_sim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mnengo_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiled_test_images\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(data[nengo_output][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\nengo\\utils\\magic.py:184\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper(wrapped, instance, args, kwargs)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nengo_dl\\simulator.py:59\u001b[0m, in \u001b[0;36mrequire_open\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"A decorator that can be used to mark methods that require the Simulator to be\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mopen.\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SimulatorClosed(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after simulator is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mSimulatorClosed\u001b[0m: Cannot call predict after simulator is closed"
     ]
    }
   ],
   "source": [
    "tiled_test_images = preprocess_img_cnn_nengo(x_data[1])\n",
    "nengo_sim, nengo_input, nengo_output = load_nengo_cnn()\n",
    "prediction = predict_img_cnn_nengo(nengo_sim, nengo_input, nengo_output, tiled_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(\n",
    "    activation,\n",
    "    params_file=\"keras_to_snn_big_params\",\n",
    "    n_steps=30,\n",
    "    scale_firing_rates=1,\n",
    "    synapse=None,\n",
    "    n_test=400,\n",
    "):\n",
    "    # convert the keras model to a nengo network\n",
    "    nengo_converter = nengo_dl.Converter(\n",
    "        model,\n",
    "        swap_activations={tf.nn.relu: activation},\n",
    "        scale_firing_rates=scale_firing_rates,\n",
    "        synapse=synapse,\n",
    "    )\n",
    "\n",
    "    # get input/output objects\n",
    "    nengo_input = nengo_converter.inputs[inp]\n",
    "    nengo_output = nengo_converter.outputs[dense]\n",
    "\n",
    "    # add a probe to the first convolutional layer to record activity.\n",
    "    # we'll only record from a subset of neurons, to save memory.\n",
    "    sample_neurons = np.linspace(\n",
    "        0,\n",
    "        np.prod(conv0.shape[1:]),\n",
    "        1000,\n",
    "        endpoint=False,\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "    with nengo_converter.net:\n",
    "        conv0_probe = nengo.Probe(nengo_converter.layers[conv0][sample_neurons])\n",
    "\n",
    "    # repeat inputs for some number of timesteps\n",
    "    tiled_test_images = np.tile(x_test[:n_test], (1, n_steps, 1))\n",
    "\n",
    "    # set some options to speed up simulation\n",
    "    with nengo_converter.net:\n",
    "        nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    # build network, load in trained weights, run inference on test images\n",
    "    with nengo_dl.Simulator(\n",
    "        nengo_converter.net, minibatch_size=10, progress_bar=False\n",
    "    ) as nengo_sim:\n",
    "        nengo_sim.load_params(params_file)\n",
    "        data = nengo_sim.predict({nengo_input: tiled_test_images})\n",
    "\n",
    "    # compute accuracy on test data, using output of network on\n",
    "    # last timestep\n",
    "    predictions = np.argmax(data[nengo_output][:, -1], axis=-1)\n",
    "    accuracy = (predictions == y_test[:n_test, 0, 0]).mean()\n",
    "    print(f\"Test accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "    # plot the results\n",
    "    for ii in range(3):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Input image\")\n",
    "        plt.imshow(x_test[ii, 0].reshape((60, 160)), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        scaled_data = data[conv0_probe][ii] * scale_firing_rates\n",
    "        if isinstance(activation, nengo.SpikingRectifiedLinear):\n",
    "            scaled_data *= 0.001\n",
    "            rates = np.sum(scaled_data, axis=0) / (n_steps * nengo_sim.dt)\n",
    "            plt.ylabel(\"Number of spikes\")\n",
    "        else:\n",
    "            rates = scaled_data\n",
    "            plt.ylabel(\"Firing rates (Hz)\")\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.title(\n",
    "            f\"Neural activities (conv0 mean={rates.mean():.1f} Hz, \"\n",
    "            f\"max={rates.max():.1f} Hz)\"\n",
    "        )\n",
    "        plt.plot(scaled_data)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Output predictions\")\n",
    "        plt.plot(tf.nn.softmax(data[nengo_output][ii]))\n",
    "        plt.legend([str(j) for j in range(10)], loc=\"upper left\")\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "\n",
    "        plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
